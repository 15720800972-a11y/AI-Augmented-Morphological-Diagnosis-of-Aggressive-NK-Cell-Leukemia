import torch
import torch.nn as nn
import numpy as np
import re
import shutil
from torch.utils.data import Dataset, DataLoader, random_split, Subset
from torchvision import transforms, models
import os
from PIL import Image
import pandas as pd
import argparse
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import accuracy_score


# é…ç½®å‚æ•°è®¾ç½®ï¼ˆå¤‡æ³¨ï¼šæƒé‡æ¨¡å‹å®šç¨¿ï¼ï¼‰
def get_config():
    parser = argparse.ArgumentParser(description="ç—…ç†å›¾åƒåˆ†æç³»ç»Ÿ")
    parser.add_argument("--data_root", type=str, default="C:/Users/bio-032/Desktop/NK",
                        help="æ•°æ®æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--label_file", type=str, default="labels.csv",
                        help="æ ‡ç­¾æ–‡ä»¶å")
    parser.add_argument("--batch_size", type=int, default=2,
                        help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--input_size", type=int, default=224,
                        help="å›¾åƒè¾“å…¥å°ºå¯¸")
    parser.add_argument("--lr", type=float, default=1e-4,
                        help="å­¦ä¹ ç‡")
    parser.add_argument("--num_epochs", type=int, default=20,
                        help="è®­ç»ƒè½®æ¬¡")
    parser.add_argument("--val_ratio", type=float, default=0.2,
                        help="éªŒè¯é›†æ¯”ä¾‹")
    parser.add_argument("--save_dir", type=str, default="C:/Users/bio-032/Desktop//saved_models",
                        help="æ¨¡å‹ä¿å­˜è·¯å¾„")
    parser.add_argument("--top_cells_dir", type=str, default="C:/Users/bio-032/Desktop/top_cells",
                        help="é«˜æƒé‡ç»†èƒä¿å­˜è·¯å¾„")
    parser.add_argument("--top_k", type=int, default=5,
                        help="æ¯ä¸ªç—…ä¾‹ä¿å­˜çš„æœ€é«˜æƒé‡ç»†èƒæ•°é‡")
    parser.add_argument("--device", type=str,
                        default="cuda" if torch.cuda.is_available() else "cpu",
                        help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--max_cells_per_patient", type=int, default=100,
                        help="æ¯ä¸ªç—…ä¾‹å¤„ç†çš„æœ€å¤§ç»†èƒæ•°")
    return parser.parse_args()



# åŒ»å­¦å½±åƒæ•°æ®é›†ç±»


class MedicalDataset(Dataset):
    def __init__(self, config):
        self.config = config
        self.label_path = os.path.join(config.data_root, config.label_file)
        self.label_df = self._load_labels()
        self.patient_dirs = self._get_patient_paths()

        # æ”¹è¿›åçš„é¢„å¤„ç†æµç¨‹
        self.transform = transforms.Compose([
            # ä¿æŒæ¯”ä¾‹çš„ç¼©æ”¾å’Œå¡«å……
            transforms.Lambda(lambda img: self._preserve_ratio_resize(img)),

            # é¢œè‰²å¢å¼ºï¼ˆç—…ç†å›¾åƒé€‚ç”¨å‚æ•°ï¼‰
            transforms.ColorJitter(
                brightness=0.2,
                contrast=0.2,
                saturation=0.2,
                hue=0.02
            ),

            # ç©ºé—´å¢å¼º
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.5),
            transforms.RandomRotation(15, fill=0),

            # è½¬æ¢ä¸ºå¼ é‡
            transforms.ToTensor(),

            # æ ‡å‡†åŒ–ï¼ˆå»ºè®®æ ¹æ®å®é™…æ•°æ®é‡æ–°è®¡ç®—ï¼‰
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),

            # éšæœºæ“¦é™¤
            transforms.RandomErasing(
                p=0.3,
                scale=(0.02, 0.1),
                ratio=(0.3, 3.3),
                value='random'
            )
        ])

    def _preserve_ratio_resize(self, img):
        """ä¿æŒé•¿å®½æ¯”çš„ç¼©æ”¾å‡½æ•°"""
        original_width, original_height = img.size
        target_size = self.config.input_size

        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        scale = min(target_size / original_width, target_size / original_height)
        new_width = int(original_width * scale)
        new_height = int(original_height * scale)

        # é«˜è´¨é‡ç¼©æ”¾
        img = img.resize(
            (new_width, new_height),
            resample=Image.LANCZOS
        )

        # åˆ›å»ºé»‘è‰²ç”»å¸ƒ
        canvas = Image.new("RGB", (target_size, target_size), (0, 0, 0))

        # å±…ä¸­ç²˜è´´
        paste_x = (target_size - new_width) // 2
        paste_y = (target_size - new_height) // 2
        canvas.paste(img, (paste_x, paste_y))

        return canvas

    def _load_labels(self):
        df = pd.read_csv(self.label_path)
        assert {'patient_id', 'ANKL'}.issubset(df.columns), "CSVç¼ºå°‘å¿…è¦åˆ—"
        return df

    def _get_patient_paths(self):
        return [os.path.join(self.config.data_root, f"patient_{row['patient_id']}")
                for _, row in self.label_df.iterrows()]

    def __len__(self):
        return len(self.patient_dirs)

    def __getitem__(self, idx):
        patient_dir = self.patient_dirs[idx]
        patient_id = re.search(r'patient_(\d+)', os.path.basename(patient_dir)).group(1).zfill(3)

        img_files = sorted([f for f in os.listdir(patient_dir) if f.endswith(('.jpg', '.png'))])
        cells = []
        cell_paths = []

        for f in img_files[:self.config.max_cells_per_patient]:  # å»ºè®®é…ç½®åŒ–
            img_path = os.path.join(patient_dir, f)
            try:
                img = Image.open(img_path).convert('RGB')
                # åº”ç”¨æ”¹è¿›åçš„transform
                cells.append(self.transform(img))
                cell_paths.append(img_path)
            except Exception as e:
                print(f"Error loading {img_path}: {str(e)}")
                continue

        return {
            'cells': cells,
            'label': torch.tensor(self.label_df.iloc[idx].ANKL, dtype=torch.float32),
            'patient_id': patient_id,
            'cell_paths': cell_paths
        }


# æ•°æ®æ•´ç†å‡½æ•°
def mil_collate_fn(batch):
    """ä¿®æ­£åçš„æ•°æ®æ•´ç†å‡½æ•°"""
    return {
        # æ¯ä¸ªç—…ä¾‹çš„ç»†èƒå›¾åƒè½¬æ¢ä¸ºå¼ é‡ [num_cells, C, H, W]
        'cells': [torch.stack(item['cells']) for item in batch],  # æ–°å¢torch.stack
        'label': torch.stack([item['label'] for item in batch]),
        'patient_ids': [item['patient_id'] for item in batch],
        'cell_paths': [item['cell_paths'] for item in batch]
    }

# æ”¹è¿›çš„MILæ¨¡å‹ï¼ˆå¸¦æ³¨æ„åŠ›æƒé‡ï¼‰
class SimplifiedMIL(nn.Module):
    def __init__(self):
        super().__init__()
        resnet = models.resnet18(pretrained=True)
        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])

        # æ³¨æ„åŠ›åˆ†æ”¯
        self.attention = nn.Sequential(
            nn.Linear(512, 128),
            nn.Tanh(),
            nn.Linear(128, 1)
        )

        # åˆ†ç±»å™¨ï¼ˆç§»é™¤æœ€åçš„Sigmoidï¼‰
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512, 1)
        )

    def forward(self, x_list):
        batch_preds = []
        batch_weights = []

        for cells in x_list:  # cellsçš„å½¢çŠ¶: [num_cells, C, H, W]
            device = next(self.parameters()).device

            # åˆ†å—å¤„ç†ç‰¹å¾ï¼ˆç›´æ¥å¤„ç†å¼ é‡ï¼‰
            all_features = []
            num_cells = cells.size(0)

            for i in range(0, num_cells, 32):
                chunk = cells[i:i + 32].to(device)
                features = self.feature_extractor(chunk).squeeze()
                all_features.append(features)

            features = torch.cat(all_features)  # å½¢çŠ¶: [num_cells, 512]

            # è®¡ç®—æ³¨æ„åŠ›æƒé‡
            attn_weights = self.attention(features)  # [num_cells, 1]
            attn_weights = torch.softmax(attn_weights, dim=0)

            # åŠ æƒèšåˆ
            aggregated = (features * attn_weights).sum(dim=0)  # [512]

            # åˆ†ç±»é¢„æµ‹
            pred = self.classifier(aggregated)  # [1]

            batch_preds.append(pred)
            batch_weights.append(attn_weights.detach().cpu().numpy())

        return torch.stack(batch_preds).view(-1), batch_weights


# ä¿å­˜é«˜æƒé‡ç»†èƒ
def save_top_cells(config, model, loader):
    model.eval()
    os.makedirs(config.top_cells_dir, exist_ok=True)

    with torch.no_grad():
        for batch in loader:
            cells_list = [cells.to(config.device) for cells in batch['cells']]
            _, batch_weights = model(cells_list)

            for i, (weights, paths) in enumerate(zip(batch_weights, batch['cell_paths'])):
                patient_id = batch['patient_ids'][i]
                weights = weights.squeeze()

                # è·å–top-kç´¢å¼•
                top_indices = np.argsort(weights)[-config.top_k:]

                # åˆ›å»ºç—…ä¾‹ç›®å½•
                patient_dir = os.path.join(config.top_cells_dir, f"patient_{patient_id}")
                os.makedirs(patient_dir, exist_ok=True)

                # å¤åˆ¶å›¾åƒ
                for idx in top_indices:
                    src_path = paths[idx]
                    dst_path = os.path.join(patient_dir, os.path.basename(src_path))
                    shutil.copy2(src_path, dst_path)

    print(f"\nâœ… é«˜æƒé‡ç»†èƒå·²ä¿å­˜è‡³: {os.path.abspath(config.top_cells_dir)}")


# è®­ç»ƒéªŒè¯å‡½æ•°
def train_model(config):
    # åˆå§‹åŒ–
    device = torch.device(config.device)
    os.makedirs(config.save_dir, exist_ok=True)

    # æ•°æ®é›†å‡†å¤‡
    full_dataset = MedicalDataset(config)

    # åˆ†å±‚åˆ’åˆ†æ•°æ®é›†
    labels = [full_dataset[i]['label'].item() for i in range(len(full_dataset))]
    sss = StratifiedShuffleSplit(n_splits=1, test_size=config.val_ratio, random_state=42)
    train_idx, val_idx = next(sss.split(np.zeros(len(labels)), labels))

    train_dataset = Subset(full_dataset, train_idx)
    val_dataset = Subset(full_dataset, val_idx)

    train_loader = DataLoader(train_dataset, batch_size=config.batch_size,
                              shuffle=True, collate_fn=mil_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size,
                            collate_fn=mil_collate_fn)

    # æ¨¡å‹é…ç½®
    model = SimplifiedMIL().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)
    criterion = nn.BCELoss()

    # è®­ç»ƒå¾ªç¯
    best_acc = 0.0
    for epoch in range(config.num_epochs):
        model.train()
        train_preds, train_labels = [], []

        for batch in train_loader:
            cells_list = [cells.to(device) for cells in batch['cells']]
            labels = batch['label'].to(device)

            optimizer.zero_grad()
            outputs, _ = model(cells_list)
            loss = criterion(torch.sigmoid(outputs), labels)
            loss.backward()
            optimizer.step()

            train_preds.append(outputs.detach().cpu())
            train_labels.append(labels.cpu())

        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_preds = torch.cat(train_preds).numpy()
        train_labels = torch.cat(train_labels).numpy()
        train_loss = loss.item()
        train_acc = accuracy_score(train_labels, train_preds > 0.5)

        # éªŒè¯é˜¶æ®µ
        val_loss, val_acc = evaluate(model, val_loader, device, criterion)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'config': vars(config)
            }, os.path.join(config.save_dir, 'best_model.pth'))

        print(f"Epoch {epoch + 1}/{config.num_epochs}")
        print(f"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\n")

    # ä¿å­˜é«˜æƒé‡ç»†èƒ
    model.load_state_dict(torch.load(os.path.join(config.save_dir, 'best_model.pth'))['model_state_dict'])
    full_loader = DataLoader(full_dataset, batch_size=config.batch_size, collate_fn=mil_collate_fn)
    save_top_cells(config, model, full_loader)

    return model


# è¯„ä¼°å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
def evaluate(model, loader, device, criterion):
    model.eval()
    preds, labels, loss_list = [], [], []  # æ·»åŠ loss_liståˆå§‹åŒ–

    with torch.no_grad():
        for batch in loader:
            cells_list = [cells.to(device) for cells in batch['cells']]
            labels_batch = batch['label'].to(device)

            outputs, _ = model(cells_list)
            loss = criterion(torch.sigmoid(outputs), labels_batch)

            loss_list.append(loss.item())  # æ”¶é›†æ¯ä¸ªbatchçš„loss
            preds.append(outputs.detach().cpu())
            labels.append(labels_batch.cpu())

    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = np.mean(loss_list) if len(loss_list) > 0 else 0.0
    acc = accuracy_score(torch.cat(labels).numpy(), torch.cat(preds).numpy() > 0.5)
    return avg_loss, acc


if __name__ == "__main__":
    config = get_config()
    print("\n" + "=" * 50)
    print(f"ğŸ¥ ç—…ç†åˆ†æç³»ç»Ÿå¯åŠ¨")
    print(f"ğŸ“‚ æ•°æ®è·¯å¾„: {os.path.abspath(config.data_root)}")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {os.path.abspath(config.save_dir)}")
    print(f"ğŸ” é«˜æƒé‡ç»†èƒä¿å­˜è·¯å¾„: {os.path.abspath(config.top_cells_dir)}")
    print("=" * 50 + "\n")

    train_model(config)