import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
import shutil
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import argparse


# ====== 1. MoCo æ¨¡å‹å®šä¹‰ (ä¿®å¤ç‰ˆ) ======
class MoCo(nn.Module):
    def __init__(self, base_encoder='resnet50', dim=128, K=8192, m=0.999, T=0.07):
        super().__init__()
        # ä½¿ç”¨ ResNet50
        base = models.__dict__[base_encoder](weights="IMAGENET1K_V1")

        # Query Encoder (å»é™¤æœ€åçš„å…¨è¿æ¥å±‚)
        self.encoder_q = nn.Sequential(
            *list(base.children())[:-1],
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten()
        )
        # Key Encoder
        self.encoder_k = nn.Sequential(
            *list(base.children())[:-1],
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten()
        )

        with torch.no_grad():
            dummy = torch.randn(2, 3, 224, 224)
            feature_dim = self.encoder_q(dummy).shape[1]

        # æŠ•å½±å¤´
        self.proj_q = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.ReLU(),
            nn.Linear(256, dim)
        )
        self.proj_k = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.ReLU(),
            nn.Linear(256, dim)
        )

        self.m = m
        self.T = T
        self.K = K

        # åˆå§‹åŒ– Key Encoder
        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):
            param_k.data.copy_(param_q.data)
            param_k.requires_grad = False
        for param_q, param_k in zip(self.proj_q.parameters(), self.proj_k.parameters()):
            param_k.data.copy_(param_q.data)
            param_k.requires_grad = False

        # é˜Ÿåˆ—
        self.register_buffer("queue", torch.randn(dim, K))
        self.queue = nn.functional.normalize(self.queue, dim=0)
        self.register_buffer("queue_ptr", torch.zeros(1, dtype=torch.long))

    @torch.no_grad()
    def _momentum_update_key_encoder(self):
        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):
            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)
        for param_q, param_k in zip(self.proj_q.parameters(), self.proj_k.parameters()):
            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)

    # è®­ç»ƒæ—¶çš„å‰å‘ä¼ æ’­
    def forward(self, im_q, im_k):
        q = self.proj_q(self.encoder_q(im_q))
        q = F.normalize(q, dim=1)

        with torch.no_grad():
            self._momentum_update_key_encoder()
            k = self.proj_k(self.encoder_k(im_k))
            k = F.normalize(k, dim=1)

        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)
        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])
        logits = torch.cat([l_pos, l_neg], dim=1)
        logits /= self.T
        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(im_q.device)

        # ç®€å•æ›´æ–°é˜Ÿåˆ— (ç®€åŒ–ç‰ˆ)
        batch_size = k.shape[0]
        ptr = int(self.queue_ptr)
        if ptr + batch_size <= self.K:
            self.queue[:, ptr:ptr + batch_size] = k.T
            ptr = (ptr + batch_size) % self.K
        else:
            self.queue[:, :-batch_size] = self.queue[:, batch_size:].clone()
            self.queue[:, -batch_size:] = k.T.detach()
            ptr = 0
        self.queue_ptr[0] = ptr

        return nn.CrossEntropyLoss()(logits, labels)

    # æ–°å¢ï¼šä¸“é—¨ç”¨äºæ¨ç†è§£å‹ç‰¹å¾çš„å‡½æ•°
    def extract_features(self, x):
        feat = self.encoder_q(x)
        return F.normalize(feat, dim=1)


# ====== 2. å¯è§†åŒ–ç±» (ä¿®å¤é‡å¤å®šä¹‰) ======
class VisualEvaluator:
    def __init__(self, output_dir):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        sns.set(style="whitegrid")

    def plot_tsne(self, features, labels, title="t-SNE Visualization"):
        plt.figure(figsize=(10, 8))
        print("æ­£åœ¨è¿›è¡Œ PCA é™ç»´...")
        pca = PCA(n_components=min(50, features.shape[1]))
        reduced_features = pca.fit_transform(features)

        print("æ­£åœ¨è¿›è¡Œ t-SNE...")
        tsne = TSNE(n_components=2, perplexity=30, random_state=42)
        embeddings = tsne.fit_transform(reduced_features)

        unique_labels = list(set(labels))
        colors = sns.color_palette("Set2", n_colors=len(unique_labels))

        for i, label in enumerate(unique_labels):
            idxs = [idx for idx, l in enumerate(labels) if l == label]
            label_name = "Unlabeled Pool" if label == 0 else "Annotated (Query)"
            plt.scatter(
                embeddings[idxs, 0], embeddings[idxs, 1],
                color=colors[i], label=label_name, alpha=0.6, s=15
            )

        plt.legend()
        plt.title(title)
        plt.savefig(os.path.join(self.output_dir, "tsne_visualization.png"), dpi=300)
        plt.close()

    def plot_similarity_distribution(self, similarities, threshold):
        plt.figure(figsize=(10, 6))
        sns.histplot(similarities, bins=50, kde=True, color="royalblue")
        plt.axvline(threshold, color='tomato', linestyle='--', label=f'Threshold: {threshold:.2f}')
        plt.title("Cosine Similarity Distribution")
        plt.xlabel("Similarity Score")
        plt.ylabel("Count")
        plt.legend()
        plt.savefig(os.path.join(self.output_dir, "similarity_dist.png"), dpi=300)
        plt.close()

    def plot_retrieval_examples(self, query_img, result_imgs, idx):
        # ç¡®ä¿ result_imgs ä¸è¶…è¿‡5å¼ 
        current_results = result_imgs[:5]
        cols = len(current_results) + 1

        fig, ax = plt.subplots(1, cols, figsize=(3 * cols, 3))

        # ç”» Query
        ax[0].imshow(query_img.resize((224, 224)))
        ax[0].axis('off')
        ax[0].set_title("Query Image", fontsize=10, color='blue')

        # ç”» Results
        for i, img in enumerate(current_results):
            ax[i + 1].imshow(img.resize((224, 224)))
            ax[i + 1].axis('off')
            ax[i + 1].set_title(f"Top {i + 1}", fontsize=10)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, f"retrieval_group_{idx}.png"), dpi=300)
        plt.close()


# ====== 3. ç‰¹å¾æå–å™¨ (ä¿®å¤é€»è¾‘é”™è¯¯) ======
class FeatureExtractor:
    def __init__(self, checkpoint_path, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = MoCo(base_encoder='resnet50', dim=128).to(self.device)

        # åŠ è½½æƒé‡
        if os.path.exists(checkpoint_path):
            print(f"Loading checkpoint from {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            # å¤„ç† state_dict é”®åå¯èƒ½å¸¦ 'module.' å‰ç¼€çš„é—®é¢˜
            state_dict = checkpoint.get('model_state_dict', checkpoint)
            # å¦‚æœ MoCo å­˜çš„æ—¶å€™åªæœ‰ encoder_qï¼Œéœ€æŒ‰éœ€åŠ è½½
            # è¿™é‡Œå‡è®¾åŠ è½½å®Œæ•´ MoCo æƒé‡ï¼Œç¨ä½œå®½å®¹å¤„ç†
            try:
                self.model.load_state_dict(state_dict, strict=False)
            except Exception as e:
                print(f"Warning loading weights: {e}")
        else:
            print("Warning: Checkpoint not found, using random weights!")

        self.model.eval()

    def extract(self, dataloader):
        features = []
        paths = []
        print("å¼€å§‹æå–ç‰¹å¾...")
        with torch.no_grad():
            for i, (images, batch_paths) in enumerate(dataloader):
                images = images.to(self.device)
                # !!! å…³é”®ä¿®æ­£ï¼šè°ƒç”¨ extract_features è€Œä¸æ˜¯ forward !!!
                feats = self.model.extract_features(images)
                features.append(feats.cpu())
                paths.extend(batch_paths)
                if i % 10 == 0:
                    print(f"Processed batch {i}/{len(dataloader)}")
        return torch.cat(features, dim=0), paths


# ====== 4. æ•°æ®é›†å®šä¹‰ ======
class SearchDataset(Dataset):
    def __init__(self, root):
        self.img_paths = [os.path.join(root, f) for f in os.listdir(root)
                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        self.transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def __getitem__(self, idx):
        path = self.img_paths[idx]
        try:
            img = Image.open(path).convert('RGB')
            return self.transform(img), path
        except:
            # å®¹é”™å¤„ç†
            return torch.zeros(3, 224, 224), path

    def __len__(self):
        return len(self.img_paths)


# ====== 5. ä¸»é…ç½®ä¸æµç¨‹ (ä¿®å¤è·¯å¾„) ======
class SearchConfig:
    # è·¯å¾„æ”¹ä¸ºç›¸å¯¹è·¯å¾„
    data_path = "./unlabeled_pool"  # æœªæ ‡æ³¨çš„å¤§æ± å­
    annotated_dir = "./annotated_query"  # å·²æ ‡æ³¨çš„ç§å­æ•°æ®ï¼ˆç”¨æ¥åšQueryï¼‰
    output_dir = "./retrieval_results"  # ç»“æœè¾“å‡º
    checkpoint_path = "./saved_models/moco_best.pth"  # MoCo æƒé‡


def find_similar_images(config):
    # 0. å‡†å¤‡å·¥ä½œ
    os.makedirs(config.output_dir, exist_ok=True)
    extractor = FeatureExtractor(config.checkpoint_path)

    # 1. æå–æœªæ ‡æ³¨æ± å­çš„ç‰¹å¾
    print(f"Loading Unlabeled Pool: {config.data_path}")
    full_dataset = SearchDataset(config.data_path)
    if len(full_dataset) == 0:
        print("Error: Unlabeled dataset is empty.")
        return
    full_loader = DataLoader(full_dataset, batch_size=64, shuffle=False, num_workers=0)
    all_features, all_paths = extractor.extract(full_loader)

    # 2. æå–ç§å­ï¼ˆQueryï¼‰å›¾ç‰‡çš„ç‰¹å¾
    print(f"Loading Query Set: {config.annotated_dir}")
    annotated_dataset = SearchDataset(config.annotated_dir)
    if len(annotated_dataset) == 0:
        print("Error: Query dataset is empty.")
        return
    annotated_loader = DataLoader(annotated_dataset, batch_size=64, shuffle=False)
    anno_features, anno_paths = extractor.extract(annotated_loader)

    # 3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ (Cosine Similarity)
    # å› ä¸ºæˆ‘ä»¬åœ¨ extract_features é‡Œå·²ç»åšäº† l2 normalizeï¼Œæ‰€ä»¥ç›´æ¥çŸ©é˜µç›¸ä¹˜å°±æ˜¯ cosine similarity
    print("Calculating Similarity Matrix...")
    sim_matrix = torch.mm(all_features, anno_features.T)  # [N_pool, N_query]

    # æ‰¾åˆ°æ¯å¼ æœªæ ‡æ³¨å›¾ç‰‡ä¸æ‰€æœ‰ Query å›¾ç‰‡çš„æœ€å¤§ç›¸ä¼¼åº¦
    max_sim, _ = torch.max(sim_matrix, dim=1)

    # 4. è®¾å®šé˜ˆå€¼ç­›é€‰
    threshold = torch.mean(max_sim) + 1.5 * torch.std(max_sim)  # ç¨å¾®ä¸¥æ ¼ä¸€ç‚¹ (+1.5 std)
    print(f"Threshold set to: {threshold:.4f}")

    # 5. ä¿å­˜ç»“æœ
    results = []
    anno_filenames = set([os.path.basename(p) for p in anno_paths])

    for path, score in zip(all_paths, max_sim.numpy()):
        filename = os.path.basename(path)
        # æ’é™¤æ‰å·²ç»æ˜¯ç§å­é›†é‡Œçš„å›¾
        if score >= threshold and filename not in anno_filenames:
            dest = os.path.join(config.output_dir, filename)
            shutil.copy(path, dest)
            results.append((filename, score))

    # å†™å…¥ CSV
    with open(os.path.join(config.output_dir, 'results.csv'), 'w') as f:
        f.write("filename,similarity_score\n")
        for name, score in sorted(results, key=lambda x: -x[1]):
            f.write(f"{name},{score:.4f}\n")

    # 6. å¯è§†åŒ–
    visualizer = VisualEvaluator(config.output_dir)

    # 6.1 t-SNE
    try:
        # ä¸‹é‡‡æ ·ä»¥åŠ å¿«ç”»å›¾
        sample_size = min(1000, len(all_features))
        if sample_size > 0:
            sample_idx = np.random.choice(len(all_features), sample_size, replace=False)
            combined_features = torch.cat([all_features[sample_idx], anno_features]).numpy()
            # 0 ä»£è¡¨æœªæ ‡æ³¨, 1 ä»£è¡¨ Query
            labels = [0] * len(sample_idx) + [1] * len(anno_features)
            visualizer.plot_tsne(combined_features, labels)
    except Exception as e:
        print(f"t-SNE Error: {str(e)}")

    # 6.2 ç›¸ä¼¼åº¦åˆ†å¸ƒ
    visualizer.plot_similarity_distribution(max_sim.numpy(), threshold.item())

    # 6.3 æ£€ç´¢å›¾ç¤º (å–å‰3ä¸ªQueryå±•ç¤º)
    try:
        num_queries = min(3, len(anno_paths))
        # å¯¹æ¯ä¸€ä¸ª query æ‰¾æœ€ç›¸ä¼¼çš„ pool images
        for q_idx in range(num_queries):
            # è·å–å½“å‰ query å¯¹æ‰€æœ‰ pool çš„ç›¸ä¼¼åº¦
            cur_sims = sim_matrix[:, q_idx]
            # æ’åº
            top_indices = torch.argsort(cur_sims, descending=True)[:5]

            query_img = Image.open(anno_paths[q_idx]).convert('RGB')
            result_imgs = [Image.open(all_paths[i]) for i in top_indices]

            visualizer.plot_retrieval_examples(query_img, result_imgs, q_idx)
    except Exception as e:
        print(f"Retrieval Vis Error: {str(e)}")

    print(f"\nâœ… å®Œæˆï¼æ‰¾åˆ° {len(results)} å¼ ç›¸ä¼¼å›¾åƒã€‚")
    print(f"ğŸ“‚ ç»“æœä¿å­˜åœ¨: {os.path.abspath(config.output_dir)}")


if __name__ == "__main__":
    torch.backends.cudnn.benchmark = True
    find_similar_images(SearchConfig())